# ğŸ§  Guide de RÃ©entraÃ®nement du ModÃ¨le

## ğŸ¯ Vue d'ensemble

Le systÃ¨me permet de rÃ©entraÃ®ner le modÃ¨le d'exoplanÃ¨tes sur de nouveaux datasets directement depuis l'Expert Workspace.

## ğŸ“Š Architecture du Training

### Composants

```
backend/
â”œâ”€â”€ training.py           # Logique d'entraÃ®nement
â”œâ”€â”€ training_routes.py    # API endpoints
â”œâ”€â”€ database.py           # Tables training_sessions & datasets
â”œâ”€â”€ uploads/              # Datasets uploadÃ©s
â””â”€â”€ model/                # ModÃ¨les sauvegardÃ©s
```

### Tables de Base de DonnÃ©es

**training_sessions**
- Suivi des sessions d'entraÃ®nement
- Statut : pending, training, completed, failed
- MÃ©triques de performance

**datasets**
- Datasets uploadÃ©s par workspace
- MÃ©tadonnÃ©es (taille, nombre d'Ã©chantillons)

## ğŸ“‹ Format du Dataset CSV

### Colonnes Requises

```csv
period,duration,depth,impact,snr,steff,srad,slogg,tmag,label
3.52,2.5,1000,0.5,15.2,5778,1.0,4.5,12.5,1
```

### Description des Colonnes

| Colonne | Type | UnitÃ© | Description |
|---------|------|-------|-------------|
| `period` | float | jours | PÃ©riode orbitale |
| `duration` | float | heures | DurÃ©e du transit |
| `depth` | float | ppm | Profondeur du transit |
| `impact` | float | - | ParamÃ¨tre d'impact (0-1) |
| `snr` | float | - | Rapport signal/bruit |
| `steff` | float | K | TempÃ©rature stellaire |
| `srad` | float | Râ˜‰ | Rayon stellaire |
| `slogg` | float | dex | Log g stellaire |
| `tmag` | float | mag | Magnitude T |
| `label` | int | - | **0**=CANDIDATE, **1**=CONFIRMED, **2**=FALSE POSITIVE |

### Encodage des Labels

```python
0 = CANDIDATE      # Candidat Ã  confirmer
1 = CONFIRMED      # ExoplanÃ¨te confirmÃ©e
2 = FALSE POSITIVE # Faux positif
```

## ğŸš€ API Endpoints

### 1. Upload Dataset

**POST** `/api/training/upload-dataset`

Upload un fichier CSV pour l'entraÃ®nement.

**Headers:**
```
Authorization: Bearer <clerk_token>
Content-Type: multipart/form-data
```

**Form Data:**
```
file: <CSV file>
workspace_key: abc123...
name: "My Training Dataset" (optional)
```

**Response (201):**
```json
{
  "success": true,
  "dataset": {
    "id": 1,
    "name": "My Training Dataset",
    "file_size": 12345,
    "num_samples": 1000
  },
  "message": "Dataset uploaded successfully"
}
```

### 2. List Datasets

**GET** `/api/training/datasets?workspace_key=abc123`

Liste tous les datasets d'un workspace.

**Response (200):**
```json
{
  "success": true,
  "datasets": [
    {
      "id": 1,
      "name": "Kepler Dataset",
      "file_size": 12345,
      "num_samples": 1000,
      "uploaded_at": "2025-01-05T10:00:00"
    }
  ],
  "count": 1
}
```

### 3. Start Training

**POST** `/api/training/start`

DÃ©marre l'entraÃ®nement sur un dataset.

**Request Body:**
```json
{
  "workspace_key": "abc123...",
  "dataset_id": 1,
  "model_name": "my_exoplanet_model",
  "epochs": 50
}
```

**Response (202):**
```json
{
  "success": true,
  "message": "Training started in background",
  "dataset": {...},
  "model_name": "my_exoplanet_model",
  "epochs": 50
}
```

### 4. List Training Sessions

**GET** `/api/training/sessions?workspace_key=abc123`

Liste toutes les sessions d'entraÃ®nement.

**Response (200):**
```json
{
  "success": true,
  "sessions": [
    {
      "id": 1,
      "model_name": "my_exoplanet_model",
      "dataset_size": 1000,
      "status": "completed",
      "model_path": "/path/to/model.keras",
      "metrics": "{\"accuracy\": 0.95}",
      "epochs_completed": 50,
      "created_at": "2025-01-05T10:00:00",
      "completed_at": "2025-01-05T11:30:00"
    }
  ],
  "count": 1
}
```

### 5. Get Training Session

**GET** `/api/training/session/<session_id>`

DÃ©tails d'une session spÃ©cifique.

## ğŸ”„ Workflow Complet

### 1. PrÃ©parer le Dataset

```python
# CrÃ©er un CSV avec les colonnes requises
import pandas as pd

data = {
    'period': [3.52, 5.21, 2.87],
    'duration': [2.5, 3.1, 1.8],
    'depth': [1000, 850, 1200],
    'impact': [0.5, 0.3, 0.7],
    'snr': [15.2, 18.5, 12.3],
    'steff': [5778, 6100, 5500],
    'srad': [1.0, 1.2, 0.9],
    'slogg': [4.5, 4.3, 4.6],
    'tmag': [12.5, 11.8, 13.2],
    'label': [1, 1, 0]  # 1=CONFIRMED, 0=CANDIDATE
}

df = pd.DataFrame(data)
df.to_csv('my_dataset.csv', index=False)
```

### 2. Upload via API

```bash
curl -X POST http://localhost:5000/api/training/upload-dataset \
  -H "Authorization: Bearer <token>" \
  -F "file=@my_dataset.csv" \
  -F "workspace_key=abc123" \
  -F "name=My Training Dataset"
```

### 3. DÃ©marrer l'EntraÃ®nement

```bash
curl -X POST http://localhost:5000/api/training/start \
  -H "Authorization: Bearer <token>" \
  -H "Content-Type: application/json" \
  -d '{
    "workspace_key": "abc123",
    "dataset_id": 1,
    "model_name": "improved_model",
    "epochs": 50
  }'
```

### 4. Suivre la Progression

```bash
curl -X GET "http://localhost:5000/api/training/sessions?workspace_key=abc123" \
  -H "Authorization: Bearer <token>"
```

## ğŸ—ï¸ Architecture du ModÃ¨le

### Inputs

1. **curve_input** : (batch, 2001, 1)
   - Courbe de transit gÃ©nÃ©rÃ©e automatiquement
   - CrÃ©Ã©e Ã  partir de period, duration, depth

2. **feat_input** : (batch, 6)
   - Features : impact, snr, steff, srad, slogg, tmag

### Outputs

1. **label** : (batch, 3)
   - Softmax classification
   - 3 classes : CANDIDATE, CONFIRMED, FALSE POSITIVE

2. **period_out** : (batch, 1)
   - RÃ©gression de la pÃ©riode

3. **depth_out** : (batch, 1)
   - RÃ©gression de la profondeur

### Architecture CNN

```
curve_input â†’ Conv1D(32) â†’ MaxPool â†’ Conv1D(64) â†’ MaxPool â†’ Conv1D(128) â†’ GlobalAvgPool
                                                                                â†“
feat_input â†’ Dense(64) â†’ Dropout â†’ Dense(32) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Concat
                                                                                â†“
                                                                    Dense(128) â†’ Dropout â†’ Dense(64)
                                                                                â†“
                                                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                        â†“                      â†“                      â†“
                                                   label (3)            period_out (1)          depth_out (1)
```

## ğŸ“Š MÃ©triques de Performance

### Loss Functions

```python
losses = {
    'label': 'categorical_crossentropy',  # Classification
    'period_out': 'mse',                  # RÃ©gression pÃ©riode
    'depth_out': 'mse'                    # RÃ©gression profondeur
}

loss_weights = {
    'label': 1.0,      # PrioritÃ© principale
    'period_out': 0.5,
    'depth_out': 0.5
}
```

### MÃ©triques Suivies

- **Accuracy** : PrÃ©cision de classification
- **Val Accuracy** : PrÃ©cision sur validation
- **Loss** : Perte totale
- **MAE** : Erreur absolue moyenne (rÃ©gression)

## ğŸ”§ Configuration

### ParamÃ¨tres d'EntraÃ®nement

```python
CURVE_LENGTH = 2001        # Points dans la courbe
BATCH_SIZE = 32            # Taille des batches
EPOCHS = 50                # Nombre d'Ã©poques
VALIDATION_SPLIT = 0.2     # 20% pour validation
```

### Callbacks

1. **EarlyStopping**
   - ArrÃªte si val_accuracy ne s'amÃ©liore pas pendant 10 Ã©poques
   - Restaure les meilleurs poids

2. **ReduceLROnPlateau**
   - RÃ©duit le learning rate si perte stagne
   - Factor: 0.5, Patience: 5

3. **TrainingProgressCallback**
   - Met Ã  jour la DB toutes les 5 Ã©poques

## ğŸ’¾ Sauvegarde des ModÃ¨les

### Nommage

```
model/
â”œâ”€â”€ exoplanet_model.keras              # ModÃ¨le actuel (utilisÃ© par l'API)
â”œâ”€â”€ my_model_20250105_103000.keras     # ModÃ¨le avec timestamp
â””â”€â”€ improved_model_20250105_143000.keras
```

### Chargement Automatique

Le modÃ¨le `exoplanet_model.keras` est chargÃ© automatiquement au dÃ©marrage du backend.

## ğŸ§ª Test de l'EntraÃ®nement

### 1. Utiliser l'Exemple

```bash
cd backend

# Upload l'exemple
curl -X POST http://localhost:5000/api/training/upload-dataset \
  -H "Authorization: Bearer test_user_123" \
  -F "file=@example_dataset.csv" \
  -F "workspace_key=YOUR_KEY" \
  -F "name=Example Dataset"

# DÃ©marrer l'entraÃ®nement
curl -X POST http://localhost:5000/api/training/start \
  -H "Authorization: Bearer test_user_123" \
  -H "Content-Type: application/json" \
  -d '{
    "workspace_key": "YOUR_KEY",
    "dataset_id": 1,
    "epochs": 10
  }'
```

### 2. Suivre les Logs

```bash
# Dans le terminal du backend
# Vous verrez :
ğŸš€ Starting model training...
ğŸ—ï¸ Creating model architecture...
ğŸ¯ Training for 10 epochs...
Epoch 1/10 ...
âœ… Training completed!
ğŸ’¾ Model saved to: model/exoplanet_model_20250105_103000.keras
```

## ğŸ†˜ DÃ©pannage

### "Missing required columns"

**Cause** : CSV mal formatÃ©
**Solution** : VÃ©rifiez que toutes les colonnes sont prÃ©sentes

### "Training failed"

**Cause** : DonnÃ©es invalides ou mÃ©moire insuffisante
**Solution** : 
- VÃ©rifiez les valeurs (pas de NaN)
- RÃ©duisez BATCH_SIZE si mÃ©moire insuffisante

### "Model not found"

**Cause** : ModÃ¨le pas encore sauvegardÃ©
**Solution** : Attendez la fin de l'entraÃ®nement

## ğŸ“ˆ Bonnes Pratiques

1. **Dataset BalancÃ©**
   - Ã‰quilibrez les 3 classes (CANDIDATE, CONFIRMED, FALSE POSITIVE)
   - Minimum 100 exemples par classe

2. **Validation**
   - Gardez 20% pour validation
   - Ne jamais entraÃ®ner sur les donnÃ©es de test

3. **HyperparamÃ¨tres**
   - Commencez avec 50 Ã©poques
   - Augmentez si underfitting
   - RÃ©duisez si overfitting

4. **Monitoring**
   - Surveillez val_accuracy vs accuracy
   - Si Ã©cart trop grand â†’ overfitting

Le systÃ¨me de rÃ©entraÃ®nement est **opÃ©rationnel** ! ğŸ‰
